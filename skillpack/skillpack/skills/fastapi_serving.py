"""fastapi-serving - Wrap models in FastAPI services with validation and health endpoints."""

import argparse
from datetime import datetime
from pathlib import Path
from textwrap import dedent
from typing import Any

from skillpack.utils.output import get_output_dir, write_text


def handler(args: argparse.Namespace) -> int:
    """CLI handler for fastapi-serving."""
    result = fastapi_serving_main(
        model_name=args.name,
        model_type=args.type,
        output_dir=args.output_dir,
    )

    if result.get("success"):
        print(f"✅ Generated FastAPI service in {result['output_dir']}")
        return 0
    print(f"❌ Error: {result.get('error')}")
    return 1


def register_parser(subparsers: Any) -> None:
    """Register the fastapi-serving subcommand."""
    parser = subparsers.add_parser(
        "fastapi-serving",
        help="Wrap models in FastAPI services with validation and health endpoints",
    )
    parser.add_argument(
        "--name",
        required=True,
        help="Model/service name",
    )
    parser.add_argument(
        "--type",
        choices=["sklearn", "pytorch", "tensorflow", "custom"],
        default="sklearn",
        help="Model type",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("./out/fastapi_serving"),
        help="Output directory",
    )
    parser.set_defaults(handler=handler)


def fastapi_serving_main(
    model_name: str,
    model_type: str = "sklearn",
    output_dir: Path | None = None,
) -> dict[str, Any]:
    """Generate FastAPI service code for model serving."""
    if output_dir is None:
        output_dir = get_output_dir("fastapi_serving")
    else:
        output_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Generate main app
        app_code = generate_app(model_name, model_type)
        write_text(content=app_code, filename="main.py", skill_name="fastapi_serving")

        # Generate models/schemas
        schemas_code = generate_schemas(model_name)
        write_text(content=schemas_code, filename="schemas.py", skill_name="fastapi_serving")

        # Generate Dockerfile
        dockerfile = generate_dockerfile(model_name)
        write_text(content=dockerfile, filename="Dockerfile", skill_name="fastapi_serving")

        # Generate requirements
        requirements = generate_requirements(model_type)
        write_text(content=requirements, filename="requirements.txt", skill_name="fastapi_serving")

        # Generate test file
        tests = generate_tests(model_name)
        write_text(content=tests, filename="test_api.py", skill_name="fastapi_serving")

        return {
            "success": True,
            "output_dir": str(output_dir),
            "files": ["main.py", "schemas.py", "Dockerfile", "requirements.txt", "test_api.py"],
        }

    except Exception as e:
        return {"success": False, "error": str(e)}


def generate_app(model_name: str, model_type: str) -> str:
    """Generate FastAPI application code."""
    return dedent(f'''\
        """FastAPI Model Serving: {model_name}
        
        Generated by skillpack fastapi-serving on {datetime.now().strftime("%Y-%m-%d %H:%M")}
        """

        import logging
        import time
        from contextlib import asynccontextmanager
        from typing import Any

        import uvicorn
        from fastapi import FastAPI, HTTPException, Request
        from fastapi.middleware.cors import CORSMiddleware

        from schemas import PredictRequest, PredictResponse, HealthResponse

        # Configure logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("{model_name}")

        # Global model reference
        model = None


        @asynccontextmanager
        async def lifespan(app: FastAPI):
            \"\"\"Load model on startup, cleanup on shutdown.\"\"\"
            global model
            logger.info("Loading model...")
            model = load_model()
            logger.info("Model loaded successfully")
            yield
            logger.info("Shutting down...")


        app = FastAPI(
            title="{model_name.replace('_', ' ').title()} API",
            description="Machine learning model serving endpoint",
            version="1.0.0",
            lifespan=lifespan,
        )

        # CORS middleware
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )


        def load_model():
            \"\"\"Load the ML model.\"\"\"
            # TODO: Implement model loading for {model_type}
            # Example for sklearn:
            # import joblib
            # return joblib.load("model.pkl")
            return {{"type": "{model_type}", "name": "{model_name}"}}


        @app.get("/health", response_model=HealthResponse)
        async def health_check():
            \"\"\"Health check endpoint.\"\"\"
            return HealthResponse(
                status="healthy",
                model_loaded=model is not None,
                model_name="{model_name}",
            )


        @app.get("/ready")
        async def readiness_check():
            \"\"\"Readiness check for kubernetes.\"\"\"
            if model is None:
                raise HTTPException(status_code=503, detail="Model not loaded")
            return {{"ready": True}}


        @app.post("/predict", response_model=PredictResponse)
        async def predict(request: PredictRequest):
            \"\"\"Make a prediction.\"\"\"
            start_time = time.time()
            
            try:
                # TODO: Implement prediction logic
                # prediction = model.predict(request.features)
                prediction = [0.5]  # Placeholder
                
                latency_ms = (time.time() - start_time) * 1000
                logger.info(f"Prediction completed in {{latency_ms:.2f}}ms")
                
                return PredictResponse(
                    prediction=prediction,
                    model_name="{model_name}",
                    latency_ms=latency_ms,
                )
            except Exception as e:
                logger.error(f"Prediction error: {{e}}")
                raise HTTPException(status_code=500, detail=str(e))


        @app.middleware("http")
        async def add_process_time_header(request: Request, call_next):
            \"\"\"Add processing time to response headers.\"\"\"
            start_time = time.time()
            response = await call_next(request)
            process_time = time.time() - start_time
            response.headers["X-Process-Time"] = str(process_time)
            return response


        if __name__ == "__main__":
            uvicorn.run(app, host="0.0.0.0", port=8000)
    ''')


def generate_schemas(model_name: str) -> str:
    """Generate Pydantic schemas."""
    return dedent(f'''\
        """Pydantic schemas for {model_name} API."""

        from typing import Any
        from pydantic import BaseModel, Field


        class PredictRequest(BaseModel):
            \"\"\"Prediction request schema.\"\"\"
            features: list[float] = Field(..., description="Input features for prediction")
            
            model_config = {{
                "json_schema_extra": {{
                    "examples": [
                        {{"features": [1.0, 2.0, 3.0, 4.0]}}
                    ]
                }}
            }}


        class PredictResponse(BaseModel):
            \"\"\"Prediction response schema.\"\"\"
            prediction: list[Any] = Field(..., description="Model predictions")
            model_name: str = Field(..., description="Name of the model used")
            latency_ms: float = Field(..., description="Inference latency in milliseconds")


        class HealthResponse(BaseModel):
            \"\"\"Health check response schema.\"\"\"
            status: str = Field(..., description="Service status")
            model_loaded: bool = Field(..., description="Whether the model is loaded")
            model_name: str = Field(..., description="Name of the loaded model")
    ''')


def generate_dockerfile(model_name: str) -> str:
    """Generate Dockerfile."""
    return dedent(f'''\
        # Dockerfile for {model_name} API
        FROM python:3.11-slim

        WORKDIR /app

        # Install dependencies
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt

        # Copy application
        COPY . .

        # Health check
        HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
            CMD curl -f http://localhost:8000/health || exit 1

        # Run the application
        EXPOSE 8000
        CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
    ''')


def generate_requirements(model_type: str) -> str:
    """Generate requirements.txt."""
    base = """\
fastapi>=0.100.0
uvicorn>=0.23.0
pydantic>=2.0.0
python-multipart>=0.0.6
"""
    if model_type == "sklearn":
        base += "scikit-learn>=1.3.0\njoblib>=1.3.0\n"
    elif model_type == "pytorch":
        base += "torch>=2.0.0\n"
    elif model_type == "tensorflow":
        base += "tensorflow>=2.13.0\n"
    
    return base


def generate_tests(model_name: str) -> str:
    """Generate test file."""
    return dedent(f'''\
        """Tests for {model_name} API."""

        from fastapi.testclient import TestClient
        from main import app

        client = TestClient(app)


        def test_health():
            \"\"\"Test health endpoint.\"\"\"
            response = client.get("/health")
            assert response.status_code == 200
            data = response.json()
            assert data["status"] == "healthy"


        def test_predict():
            \"\"\"Test prediction endpoint.\"\"\"
            response = client.post(
                "/predict",
                json={{"features": [1.0, 2.0, 3.0, 4.0]}}
            )
            assert response.status_code == 200
            data = response.json()
            assert "prediction" in data
            assert data["model_name"] == "{model_name}"
    ''')
