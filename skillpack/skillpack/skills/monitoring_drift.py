"""monitoring-drift - Generate data and prediction drift detection templates."""

import argparse
from datetime import datetime
from pathlib import Path
from textwrap import dedent
from typing import Any

from skillpack.utils.output import get_output_dir, write_text


def handler(args: argparse.Namespace) -> int:
    """CLI handler for monitoring-drift."""
    result = monitoring_drift_main(
        model_name=args.name,
        drift_type=args.type,
        output_dir=args.output_dir,
    )

    if result.get("success"):
        print(f"✅ Generated drift monitoring: {result['output_dir']}")
        for f in result.get("files", []):
            print(f"   - {f}")
        return 0
    print(f"❌ Error: {result.get('error')}")
    return 1


def register_parser(subparsers: Any) -> None:
    """Register the monitoring-drift subcommand."""
    parser = subparsers.add_parser(
        "monitoring-drift",
        help="Generate data and prediction drift detection templates",
    )
    parser.add_argument("--name", required=True, help="Model/dataset name")
    parser.add_argument(
        "--type",
        choices=["data", "prediction", "both"],
        default="both",
        help="Type of drift to monitor",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("./out/monitoring_drift"),
        help="Output directory",
    )
    parser.set_defaults(handler=handler)


def monitoring_drift_main(
    model_name: str,
    drift_type: str = "both",
    output_dir: Path | None = None,
) -> dict[str, Any]:
    """Generate drift monitoring code."""
    if output_dir is None:
        output_dir = get_output_dir("monitoring_drift")
    else:
        output_dir.mkdir(parents=True, exist_ok=True)

    try:
        files = []

        if drift_type in ["data", "both"]:
            data_drift = generate_data_drift_monitor(model_name)
            write_text(content=data_drift, filename="data_drift.py", skill_name="monitoring_drift")
            files.append("data_drift.py", output_dir=output_dir)

        if drift_type in ["prediction", "both"]:
            pred_drift = generate_prediction_drift_monitor(model_name)
            write_text(content=pred_drift, filename="prediction_drift.py", skill_name="monitoring_drift")
            files.append("prediction_drift.py", output_dir=output_dir)

        # Generate alerting config
        alerting = generate_alerting_config(model_name)
        write_text(content=alerting, filename="alerting.yaml", skill_name="monitoring_drift")
        files.append("alerting.yaml", output_dir=output_dir)

        # Generate dashboard template
        dashboard = generate_dashboard(model_name)
        write_text(content=dashboard, filename="dashboard.json", skill_name="monitoring_drift")
        files.append("dashboard.json", output_dir=output_dir)

        # Generate README
        readme = generate_readme(model_name, drift_type)
        write_text(content=readme, filename="README.md", skill_name="monitoring_drift")
        files.append("README.md", output_dir=output_dir)

        return {
            "success": True,
            "output_dir": str(output_dir),
            "files": files,
        }

    except Exception as e:
        return {"success": False, "error": str(e)}


def generate_data_drift_monitor(name: str) -> str:
    """Generate data drift monitoring code."""
    return dedent(f'''\
        #!/usr/bin/env python3
        """Data drift monitoring for {name}.

        Generated by skillpack on {datetime.now().strftime("%Y-%m-%d %H:%M")}
        """

        import logging
        from dataclasses import dataclass
        from pathlib import Path
        from typing import Any

        import numpy as np
        import pandas as pd
        from scipy import stats

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)


        @dataclass
        class DriftResult:
            feature: str
            statistic: float
            p_value: float
            is_drifted: bool
            drift_type: str


        class DataDriftMonitor:
            """Monitor for data distribution drift."""
            
            def __init__(
                self,
                reference_data: pd.DataFrame,
                threshold: float = 0.05,
            ):
                self.reference_data = reference_data
                self.threshold = threshold
                self.reference_stats = self._compute_stats(reference_data)
            
            def _compute_stats(self, df: pd.DataFrame) -> dict:
                \"\"\"Compute reference statistics.\"\"\"
                stats_dict = {{}}
                for col in df.columns:
                    if df[col].dtype in ["int64", "float64"]:
                        stats_dict[col] = {{
                            "mean": df[col].mean(),
                            "std": df[col].std(),
                            "min": df[col].min(),
                            "max": df[col].max(),
                            "quantiles": df[col].quantile([0.25, 0.5, 0.75]).tolist(),
                        }}
                    else:
                        stats_dict[col] = {{
                            "value_counts": df[col].value_counts(normalize=True).to_dict(),
                        }}
                return stats_dict
            
            def detect_drift(self, current_data: pd.DataFrame) -> list[DriftResult]:
                \"\"\"Detect drift in current data vs reference.\"\"\"
                results = []
                
                for col in self.reference_data.columns:
                    if col not in current_data.columns:
                        continue
                    
                    ref_col = self.reference_data[col]
                    cur_col = current_data[col]
                    
                    if ref_col.dtype in ["int64", "float64"]:
                        # Kolmogorov-Smirnov test for numerical
                        statistic, p_value = stats.ks_2samp(ref_col, cur_col)
                        drift_type = "ks_test"
                    else:
                        # Chi-square test for categorical
                        ref_counts = ref_col.value_counts()
                        cur_counts = cur_col.value_counts()
                        
                        # Align categories
                        all_cats = set(ref_counts.index) | set(cur_counts.index)
                        ref_aligned = [ref_counts.get(c, 0) for c in all_cats]
                        cur_aligned = [cur_counts.get(c, 0) for c in all_cats]
                        
                        statistic, p_value = stats.chisquare(cur_aligned, ref_aligned)
                        drift_type = "chi_square"
                    
                    is_drifted = p_value < self.threshold
                    
                    results.append(DriftResult(
                        feature=col,
                        statistic=float(statistic),
                        p_value=float(p_value),
                        is_drifted=is_drifted,
                        drift_type=drift_type,
                    ))
                    
                    if is_drifted:
                        logger.warning(f"Drift detected in {{col}}: p={{p_value:.4f}}")
                
                return results
            
            def get_drift_summary(self, results: list[DriftResult]) -> dict:
                \"\"\"Summarize drift detection results.\"\"\"
                drifted = [r for r in results if r.is_drifted]
                return {{
                    "total_features": len(results),
                    "drifted_features": len(drifted),
                    "drift_rate": len(drifted) / len(results) if results else 0,
                    "drifted_feature_names": [r.feature for r in drifted],
                }}


        # Example usage
        if __name__ == "__main__":
            # Create sample data
            np.random.seed(42)
            reference = pd.DataFrame({{
                "feature_1": np.random.normal(0, 1, 1000),
                "feature_2": np.random.normal(5, 2, 1000),
                "category": np.random.choice(["A", "B", "C"], 1000),
            }})
            
            # Simulated current data with drift
            current = pd.DataFrame({{
                "feature_1": np.random.normal(0.5, 1, 500),  # Shifted mean
                "feature_2": np.random.normal(5, 2, 500),   # No drift
                "category": np.random.choice(["A", "B", "D"], 500),  # New category
            }})
            
            monitor = DataDriftMonitor(reference)
            results = monitor.detect_drift(current)
            summary = monitor.get_drift_summary(results)
            
            print(f"Drift Summary: {{summary}}")
    ''')


def generate_prediction_drift_monitor(name: str) -> str:
    """Generate prediction drift monitoring code."""
    return dedent(f'''\
        #!/usr/bin/env python3
        """Prediction drift monitoring for {name}.

        Generated by skillpack on {datetime.now().strftime("%Y-%m-%d %H:%M")}
        """

        import logging
        from collections import deque
        from dataclasses import dataclass
        from datetime import datetime, timedelta
        from typing import Any

        import numpy as np
        from scipy import stats

        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)


        @dataclass
        class PredictionDriftResult:
            window_start: str
            window_end: str
            mean_prediction: float
            std_prediction: float
            drift_score: float
            is_drifted: bool


        class PredictionDriftMonitor:
            """Monitor for prediction distribution drift."""
            
            def __init__(
                self,
                reference_predictions: np.ndarray,
                window_size: int = 1000,
                threshold: float = 0.05,
            ):
                self.reference_predictions = reference_predictions
                self.reference_mean = np.mean(reference_predictions)
                self.reference_std = np.std(reference_predictions)
                self.window_size = window_size
                self.threshold = threshold
                
                self.prediction_buffer = deque(maxlen=window_size)
                self.timestamp_buffer = deque(maxlen=window_size)
            
            def add_prediction(self, prediction: float, timestamp: datetime | None = None) -> None:
                \"\"\"Add a new prediction to the buffer.\"\"\"
                self.prediction_buffer.append(prediction)
                self.timestamp_buffer.append(timestamp or datetime.now())
            
            def add_predictions(self, predictions: np.ndarray) -> None:
                \"\"\"Add batch of predictions.\"\"\"
                for pred in predictions:
                    self.add_prediction(pred)
            
            def check_drift(self) -> PredictionDriftResult | None:
                \"\"\"Check for drift in current prediction window.\"\"\"
                if len(self.prediction_buffer) < self.window_size // 2:
                    return None
                
                current_preds = np.array(self.prediction_buffer)
                
                # Statistical test
                statistic, p_value = stats.ks_2samp(
                    self.reference_predictions, current_preds
                )
                
                is_drifted = p_value < self.threshold
                
                if is_drifted:
                    logger.warning(f"Prediction drift detected: p={{p_value:.4f}}")
                
                return PredictionDriftResult(
                    window_start=str(min(self.timestamp_buffer)),
                    window_end=str(max(self.timestamp_buffer)),
                    mean_prediction=float(np.mean(current_preds)),
                    std_prediction=float(np.std(current_preds)),
                    drift_score=float(statistic),
                    is_drifted=is_drifted,
                )
            
            def get_psi(self, bins: int = 10) -> float:
                \"\"\"Calculate Population Stability Index (PSI).\"\"\"
                current_preds = np.array(self.prediction_buffer)
                
                # Create bins from reference
                _, bin_edges = np.histogram(self.reference_predictions, bins=bins)
                
                # Get counts for both distributions
                ref_counts, _ = np.histogram(self.reference_predictions, bins=bin_edges)
                cur_counts, _ = np.histogram(current_preds, bins=bin_edges)
                
                # Convert to proportions
                ref_prop = ref_counts / len(self.reference_predictions)
                cur_prop = cur_counts / len(current_preds)
                
                # Avoid division by zero
                ref_prop = np.clip(ref_prop, 1e-6, 1)
                cur_prop = np.clip(cur_prop, 1e-6, 1)
                
                # Calculate PSI
                psi = np.sum((cur_prop - ref_prop) * np.log(cur_prop / ref_prop))
                
                return float(psi)


        class ConceptDriftMonitor:
            \"\"\"Monitor for concept drift using ADWIN algorithm.\"\"\"
            
            def __init__(self, delta: float = 0.002):
                self.delta = delta
                self.predictions = []
                self.actuals = []
                self.errors = []
            
            def add_sample(self, prediction: float, actual: float) -> bool:
                \"\"\"Add sample and check for concept drift.\"\"\"
                self.predictions.append(prediction)
                self.actuals.append(actual)
                
                error = abs(prediction - actual)
                self.errors.append(error)
                
                # Simple drift detection: sliding window error comparison
                if len(self.errors) < 100:
                    return False
                
                recent_error = np.mean(self.errors[-50:])
                historical_error = np.mean(self.errors[-100:-50])
                
                # Significant increase in error indicates concept drift
                return recent_error > historical_error * 1.5


        # Example usage
        if __name__ == "__main__":
            np.random.seed(42)
            
            # Reference predictions
            reference = np.random.beta(2, 5, 10000)
            
            # Current predictions (drifted)
            current = np.random.beta(3, 4, 1000)
            
            monitor = PredictionDriftMonitor(reference)
            monitor.add_predictions(current)
            
            result = monitor.check_drift()
            print(f"Drift detected: {{result.is_drifted}}")
            print(f"PSI: {{monitor.get_psi():.4f}}")
    ''')


def generate_alerting_config(name: str) -> str:
    """Generate alerting configuration."""
    import yaml
    
    return yaml.dump({
        "model_name": name,
        "alerts": {
            "data_drift": {
                "enabled": True,
                "threshold": 0.05,
                "check_interval": "1h",
                "channels": ["slack", "pagerduty"],
            },
            "prediction_drift": {
                "enabled": True,
                "psi_threshold": 0.2,
                "check_interval": "30m",
                "channels": ["slack"],
            },
            "concept_drift": {
                "enabled": True,
                "error_increase_threshold": 1.5,
                "check_interval": "1h",
                "channels": ["slack", "email"],
            },
        },
        "slack_webhook": "https://hooks.slack.com/services/xxx",
        "pagerduty_key": "xxx",
    }, default_flow_style=False)


def generate_dashboard(name: str) -> str:
    """Generate Grafana dashboard template."""
    import json
    
    return json.dumps({
        "title": f"{name} Drift Monitoring",
        "panels": [
            {"title": "Data Drift Score", "type": "timeseries"},
            {"title": "PSI Over Time", "type": "timeseries"},
            {"title": "Prediction Distribution", "type": "histogram"},
            {"title": "Feature Drift Heatmap", "type": "heatmap"},
        ],
    }, indent=2)


def generate_readme(name: str, drift_type: str) -> str:
    """Generate README."""
    return dedent(f'''\
        # Drift Monitoring: {name}

        Generated by skillpack on {datetime.now().strftime("%Y-%m-%d %H:%M")}

        ## Monitoring Types
        - {"Data drift (feature distribution changes)" if drift_type in ["data", "both"] else ""}
        - {"Prediction drift (output distribution changes)" if drift_type in ["prediction", "both"] else ""}

        ## Usage

        ```python
        from data_drift import DataDriftMonitor
        from prediction_drift import PredictionDriftMonitor

        # Data drift
        monitor = DataDriftMonitor(reference_df)
        results = monitor.detect_drift(current_df)

        # Prediction drift
        pred_monitor = PredictionDriftMonitor(reference_predictions)
        pred_monitor.add_predictions(current_predictions)
        result = pred_monitor.check_drift()
        ```

        ## Metrics
        - **KS Statistic**: Kolmogorov-Smirnov test for numerical features
        - **Chi-Square**: For categorical features
        - **PSI**: Population Stability Index (>0.2 = significant drift)
    ''')
